# Benchmark Comparison Template

Use this table to compare outputs from different prompts, models, or providers.

| Run ID | Provider/Model | Clarity (1-10) | Priority Strength (1-10) | MVP Strictness (1-10) | Execution Feasibility (1-10) | Total Score (1-10000) | Notes |
|---|---|---:|---:|---:|---:|---:|---|
| baseline-openai | openai/gpt-4o-mini |  |  |  |  |  |  |
| baseline-anthropic | anthropic/claude-3-5-sonnet-latest |  |  |  |  |  |  |

## Review Checklist
- Did the run produce a sharp problem statement and clear persona?
- Is MoSCoW balanced, with explicit anti-creep exclusions?
- Is MVP strict and testable in 7 days?
- Are KPIs measurable and tied to adoption/value?
- Are risks practical with actionable mitigations?
